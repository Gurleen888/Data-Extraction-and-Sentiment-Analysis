{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING the required LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import nltk # Natural Language ToolKit, for stopping, stemming, lemmatization\n",
    "nltk.download('punkt') # default download\n",
    "nltk.download('stopwords') # downloading the stopwords to be used in the stopping process\n",
    "nltk.download('wordnet') # downloading wordnet(is like imagenet for text) text data for stemming and lemmatization purpose\n",
    "from nltk.corpus import stopwords # imports the stopwords we can use\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # Lemmatizer\n",
    "import re # Regular Expression, RegEx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing tweepy which is library for accessing the Twitter API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessed from Developer Twitter account\n",
    "API_key = 'XBH9Lt4zTpHALV1mrC0ysY5om'\n",
    "API_key_Secret = 's8MaV2KAu3Ed7SfgpU5k353p9NWpaMM3XQxgz1L53HNsotOAcq'\n",
    "Access_token = '1375000738934321153-9u09DmWasVu3LvPLWuWhgoKhdN3Bs2'\n",
    "Access_token_secret = 'MzMUxLRZUI6OmrHnBP1dNZisFdM96Ro3jdt2jgugYOYYg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(API_key,API_key_Secret)\n",
    "auth.set_access_token(Access_token,Access_token_secret)\n",
    "api=tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataframe \n",
    "df = pd.DataFrame(columns=[\"Date\",\"User\",\"IsVerified\",\"Tweet\",\"Likes\",\"RT\",'User_location'])\n",
    "df1 = pd.DataFrame(columns=[\"Date\",\"User\",\"IsVerified\",\"Tweet\",\"Likes\",\"RT\",'User_location'])\n",
    "df2 = pd.DataFrame(columns=[\"Date\",\"User\",\"IsVerified\",\"Tweet\",\"Likes\",\"RT\",'User_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting 3000 tweets from\n",
    "### 1) Amazon India\n",
    "### 2) Flipkart\n",
    "### 3) Snapdeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amazon\n",
    "def get_tweets(username):\n",
    "    i=0\n",
    "    for tweet in tweepy.Cursor(api.search, q=username, lang=\"en\",exclude='retweets').items(3000):\n",
    "        print(i, end='\\r')\n",
    "        df.loc[i,\"Date\"] = tweet.created_at\n",
    "        df.loc[i,\"User\"] = tweet.user.name\n",
    "        df.loc[i,\"IsVerified\"] = tweet.user.verified\n",
    "        df.loc[i,\"Tweet\"] = tweet.text\n",
    "        df.loc[i,\"Likes\"] = tweet.favorite_count\n",
    "        df.loc[i,\"RT\"] = tweet.retweet_count\n",
    "        df.loc[i,\"User_location\"] = tweet.user.location\n",
    "        df.to_csv(\"AmazonDataset.csv\",index=False)\n",
    "        #df.to_excel('{}.xlsx'.format(\"TweetDataset\"),index=False)   ## Save as Excel\n",
    "        i=i+1\n",
    "        if i>=3000:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "get_tweets('AmazonIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flipkart\n",
    "def get_tweets(username):\n",
    "    i=0\n",
    "    for tweet in tweepy.Cursor(api.search, q=username, lang=\"en\",exclude='retweets').items(3000):\n",
    "        print(i, end='\\r')\n",
    "        df1.loc[i,\"Date\"] = tweet.created_at\n",
    "        df1.loc[i,\"User\"] = tweet.user.name\n",
    "        df1.loc[i,\"IsVerified\"] = tweet.user.verified\n",
    "        df1.loc[i,\"Tweet\"] = tweet.text\n",
    "        df1.loc[i,\"Likes\"] = tweet.favorite_count\n",
    "        df1.loc[i,\"RT\"] = tweet.retweet_count\n",
    "        df1.loc[i,\"User_location\"] = tweet.user.location\n",
    "        df1.to_csv(\"FlipkartDataset.csv\",index=False)\n",
    "        #df.to_excel('{}.xlsx'.format(\"TweetDataset\"),index=False)   ## Save as Excel\n",
    "        i=i+1\n",
    "        if i>=3000:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "get_tweets('Flipkart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanpdeal\n",
    "def get_tweets(username):\n",
    "    i=0\n",
    "    for tweet in tweepy.Cursor(api.search, q=username, lang=\"en\",exclude='retweets').items(3000):\n",
    "        print(i, end='\\r')\n",
    "        df2.loc[i,\"Date\"] = tweet.created_at\n",
    "        df2.loc[i,\"User\"] = tweet.user.name\n",
    "        df2.loc[i,\"IsVerified\"] = tweet.user.verified\n",
    "        df2.loc[i,\"Tweet\"] = tweet.text\n",
    "        df2.loc[i,\"Likes\"] = tweet.favorite_count\n",
    "        df2.loc[i,\"RT\"] = tweet.retweet_count\n",
    "        df2.loc[i,\"User_location\"] = tweet.user.location\n",
    "        df2.to_csv(\"SnapdealDataset.csv\",index=False)\n",
    "        #df.to_excel('{}.xlsx'.format(\"TweetDataset\"),index=False)   ## Save as Excel\n",
    "        i=i+1\n",
    "        if i>=3000:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "get_tweets('snapdeal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "df = pd.read_csv('AmazonDataset.csv')\n",
    "df1 = pd.read_csv('FlipkartDataset.csv')\n",
    "df2 = pd.read_csv('SnapdealDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the non required columns\n",
    "Tweet_df = df.drop(['Date','User','IsVerified','Likes','RT','User_location'],1,inplace=True)\n",
    "Tweet_df1 = df1.drop(['Date','User','IsVerified','Likes','RT','User_location'],1,inplace=True)\n",
    "Tweet_df2 = df2.drop(['Date','User','IsVerified','Likes','RT','User_location'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Punctuations and Cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "result = string.punctuation\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(Tweet):\n",
    "    Tweet = Tweet.lower()\n",
    "    Tweet = re.sub('\\[.*?\\]', '', Tweet)\n",
    "    Tweet = re.sub('https?://\\S+|www\\.\\S+', '', Tweet)\n",
    "    Tweet = re.sub('<.*?>+', '', Tweet)\n",
    "    Tweet = re.sub('[%s]' % re.escape(string.punctuation), '', Tweet)\n",
    "    Tweet = re.sub('\\n', '', Tweet)\n",
    "    Tweet = re.sub('\\w*\\d\\w*', '', Tweet)\n",
    "    return Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_df = df['Tweet'] = df['Tweet'].apply(lambda x: clean_text(x))\n",
    "Tweet_df1 = df1['Tweet'] = df1['Tweet'].apply(lambda x: clean_text(x))\n",
    "Tweet_df2 = df2['Tweet'] = df2['Tweet'].apply(lambda x: clean_text(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Positive, Negative, Neutral Tweet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Textblob for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(Tweet):\n",
    "    analysis = TextBlob(Tweet)\n",
    "    \n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"] = df[\"Tweet\"].apply(lambda x : analyze_sentiment(x))\n",
    "df1[\"Sentiment\"] = df1[\"Tweet\"].apply(lambda x : analyze_sentiment(x))\n",
    "df2[\"Sentiment\"] = df2[\"Tweet\"].apply(lambda x : analyze_sentiment(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count for every sentiment for every tweet.\n",
    "\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "print(df1[\"Sentiment\"].value_counts())\n",
    "print(df2[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PieChart of Sentiment analysis for all three Companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=len(df[df[\"Sentiment\"]==\"Positive\"])\n",
    "b=len(df[df[\"Sentiment\"]==\"Negative\"])\n",
    "c=len(df[df[\"Sentiment\"]==\"Neutral\"])\n",
    "d=np.array([a,b,c])\n",
    "explode = (0.1, 0.0, 0.1)\n",
    "plt.pie(d,shadow=True,explode=explode,labels=[\"Positive\",\"Negative\",\"Neutral\"],autopct='%1.2f%%')\n",
    "plt.title(\"Amazon India\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=len(df1[df1[\"Sentiment\"]==\"Positive\"])\n",
    "b=len(df1[df1[\"Sentiment\"]==\"Negative\"])\n",
    "c=len(df1[df1[\"Sentiment\"]==\"Neutral\"])\n",
    "d=np.array([a,b,c])\n",
    "explode = (0.1, 0.0, 0.1)\n",
    "plt.pie(d,shadow=True,explode=explode,labels=[\"Positive\",\"Negative\",\"Neutral\"],autopct='%1.2f%%')\n",
    "plt.title(\"Flipkart\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=len(df2[df2[\"Sentiment\"]==\"Positive\"])\n",
    "b=len(df2[df2[\"Sentiment\"]==\"Negative\"])\n",
    "c=len(df2[df2[\"Sentiment\"]==\"Neutral\"])\n",
    "d=np.array([a,b,c])\n",
    "explode = (0.1, 0.0, 0.1)\n",
    "plt.pie(d,shadow=True,explode=explode,labels=[\"Positive\",\"Negative\",\"Neutral\"],autopct='%1.2f%%')\n",
    "plt.title(\"Snapdeal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(Tweet):\n",
    "    words = [w for w in Tweet if w not in stopword]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet_nonstop'] = df['Tweet'].apply(lambda x: remove_stopwords(x))\n",
    "df1['Tweet_nonstop'] = df1['Tweet'].apply(lambda x: remove_stopwords(x))\n",
    "df2['Tweet_nonstop'] = df2['Tweet'].apply(lambda x: remove_stopwords(x))\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Sentiments for the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Wordcloud\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSITIVE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AMAZON IN\n",
    "\n",
    "# POSITIVE reviews \n",
    "text_positive = \" \".join(review for review in df[df[\"Sentiment\"]==\"Positive\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_positive)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_positive)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive reviews of Amazon India')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLIPKART\n",
    "\n",
    "# POSITIVE reviews \n",
    "text_positive = \" \".join(review for review in df1[df1[\"Sentiment\"]==\"Positive\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_positive)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_positive)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive reviews of Flipkart')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNAPDEAL\n",
    "\n",
    "# POSITIVE reviews \n",
    "text_positive = \" \".join(review for review in df2[df2[\"Sentiment\"]==\"Positive\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_positive)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_positive)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive reviews of Snapdeal')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEGATIVE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AMAZON IN\n",
    "\n",
    "# Negative reviews\n",
    "text_negative = \" \".join(review for review in df[df[\"Sentiment\"]==\"Negative\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_negative)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_negative)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative reviews - Amazon India')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FLIPKART\n",
    "\n",
    "# Negative reviews\n",
    "text_negative = \" \".join(review for review in df1[df1[\"Sentiment\"]==\"Negative\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_negative)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_negative)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative reviews - Flipkart ')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNAPDEAL\n",
    "\n",
    "# Negative reviews\n",
    "text_negative = \" \".join(review for review in df2[df2[\"Sentiment\"]==\"Negative\"].Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text_negative)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=800,max_font_size=70).generate(text_negative)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative reviews - Snapdeal')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A WORDCLOUD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMAZON IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews \n",
    "text = \" \".join(review for review in df.Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=3000,max_font_size=70).generate(text)\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Reviews of Amazon India')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews \n",
    "text = \" \".join(review for review in df1.Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=3000,max_font_size=70).generate(text)\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Reviews of Flipkart')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNAPDEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews \n",
    "text = \" \".join(review for review in df2.Tweet)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.update([\"drink\", \"now\", \"wine\", \"flavor\", \"flavors\"])  #To add any custom StopWords\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_words=3000,max_font_size=70).generate(text)\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Reviews of Snapdeal')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Statement: \n",
    "    1. Extracted 3000 tweets from twitter for AMAZONIN, FLIPKART, SNAPDEAL\n",
    "    2. Removed stop words and Performed positive and negative tweet analysis.\n",
    "    3. Created  word-cloud for all three companies.\n",
    "    4. Analysed for which products what are the sentiments (by creating a wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
